{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "secret-mining",
   "metadata": {},
   "source": [
    "# CHAPTER 3: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amended-accreditation",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "together-triple",
   "metadata": {},
   "source": [
    "Predicting a quantitative response $Y$ on the basis of a single predictor variable $X$, assuming a linear relationship:\n",
    "$$\n",
    "Y \\sim \\beta_{0} + \\beta_{1}X\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-exception",
   "metadata": {},
   "source": [
    "Using training data we find estimates for the coefficient and we can predict a response value using $x$ with the formula $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-artist",
   "metadata": {},
   "source": [
    "### Estimating the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-trademark",
   "metadata": {},
   "source": [
    "There are many ways to estimate the coefficients using training data, we now see the least squares criterion: using the above \n",
    "formula with point $x_i$, we have that \n",
    "\n",
    "$e_i = y_i - \\hat{y}_i$\n",
    "\n",
    "is the $i$-th residual, and we define the residual sum of squares as $RSS = e_1^2  +  ...  +  e_n^2$.\n",
    "\n",
    "After some computation, one can show that the coefficients that minimize the $RSS$ are:\n",
    "\n",
    "- $\\hat{\\beta}_1 = \\dfrac{\\sum(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum(x_i-\\bar{x})^2}$,\n",
    "\n",
    "- $\\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adapted-stable",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of the Coefficient Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-olympus",
   "metadata": {},
   "source": [
    "The population regression line is $Y = \\beta_{0} + \\beta_{1}X + \\epsilon$.\n",
    "\n",
    "Analogy between :\n",
    "- the computed regression line and the population regression line\n",
    "\n",
    "vs\n",
    "\n",
    "- the sample mean and the population mean\n",
    "\n",
    "e.g., we know that the variance of $\\hat{\\mu}$ is $\\text{Var}(\\hat{\\mu}) = SE(\\hat{\\mu})^2 = \\dfrac{\\sigma^2}{n}$, where $\\sigma$ is the standard deviation of the response training data. The standard error tells how much on average the estimated mean differs from the population mean.\n",
    "Analogously, the standard errors of the estimated coefficients are:\n",
    "\n",
    "- $SE(\\hat{\\beta}_0)^2 = \\sigma^2\\left[\\dfrac{1}{n} + \\dfrac{\\bar{x}^2}{\\sum(x_i-\\bar{x})^2}\\right]$,\n",
    "- $SE(\\hat{\\beta}_1)^2 = \\dfrac{\\sigma^2}{\\sum(x_i-\\bar{x})^2}$,\n",
    "\n",
    "where $\\sigma^2 = \\text{Var}(\\epsilon)$. The errors $e_i$ for each observation should be uncorrelated and have common variance, but the formula remains a good approximation.\n",
    "\n",
    "Generally, $\\sigma^2$ is unknown. The estimate of $\\sigma$ is called the **residual standard error**: $RSE = \\sqrt{\\dfrac{RSS}{n-2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-tackle",
   "metadata": {},
   "source": [
    "#### Use of standard error for Confidence Intervals and Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-technique",
   "metadata": {},
   "source": [
    "95% confidence interval of $\\hat{\\beta}_1$ is approximately $\\hat{\\beta}_1 \\pm 2 \\cdot SE(\\hat{\\beta}_1)$, and the same holds for $\\hat{\\beta}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selective-penguin",
   "metadata": {},
   "source": [
    "Hypothesis test on the coefficients, where null hypothesis is: NO relationship between $X$ and $Y$, corresponding to $\\beta_1 = 0$.\n",
    "To test the null hypothesis, we need to determine whether $\\hat{\\beta}_1$, our estimate for $\\beta_1$, is sufficiently far from zero that we can be confident that $\\beta_1$ is non-zero. To do so, a $t$-statistic is computed, \n",
    "$$\n",
    "t = \\dfrac{\\hat{\\beta}_1 - 0}{SE(\\hat{\\beta}_1)},\n",
    "$$\n",
    "which measures the number of standard deviations that $\\hat{\\beta}_1$ is away from 0. If there is a relationship, we expect the equation above to have a $t$-distributon with $n-2$ degrees of freedom, so it is a matter of computing the probability of observing any number $>= |t|$, assuming that $\\beta_1 = 0$, and this probability is called $p$-value. If the $p$-value is sufficiently small, we can reject the null hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-beach",
   "metadata": {},
   "source": [
    "### Assessing the accuracy of the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-tanzania",
   "metadata": {},
   "source": [
    "The extent to which the model fits the data; the quality of a linear regression fit is usually assessed using the **residual standard error** (RSE) and the $R^2$ statistic.\n",
    "\n",
    "- The RSE is an estimate of the standard deviation of $\\epsilon$, roughly speaking, it is the average amount that the response will deviate from the true regression line. Another way to think about this is that even if the model were correct and the true values of the unknown coefficients $\\beta_0$ and $\\beta_1$ were known exactly, any prediction would still be off by the RSE.\n",
    "\n",
    "- $R^2 = \\dfrac{\\text{TSS - RSS}}{\\text{TSS}} = 1- \\dfrac{\\text{RSS}}{\\text{TSS}}$, where TSS = $\\sum(y_i - \\bar{y})^2$. TSS measures the total variance in the response $Y$, and can be thought of as the amount of variability inherent in the response before the regression is performed. In contrast, RSS measures the amount of variability that is left unexplained after performing the regression. Hence, TSS âˆ’ RSS measures the amount of variability in the response that is explained (or removed) by performing the regression, and $R^2$ measures the proportion of variability in $Y$ that can be explained using $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-window",
   "metadata": {},
   "source": [
    "Only in the simple linear regression, $R^2 = r^2$, where $r$ is the sample correlation between $X$ and $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-buyer",
   "metadata": {},
   "source": [
    "## Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-storm",
   "metadata": {},
   "source": [
    "$Y = \\beta_0 + \\beta_1X_1 + ... + \\beta_pX_p + \\epsilon$\n",
    "\n",
    "We interpret $\\beta_j$ as the average effect on $Y$ of a one unit increase in $X_j$, holding all other predictors fixed.\n",
    "Coefficients are estimated as before, i.e. we choose $\\hat{\\beta}_j$'s that minimize the RSS, but with more complicated linear algebra formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-north",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimum-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-replication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-reform",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
