{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "loose-lawsuit",
   "metadata": {},
   "source": [
    "# Chapter 4: Classification\n",
    "Predicting qualitative responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordinary-accessory",
   "metadata": {},
   "source": [
    "Linear regression is not suited because any encoding implies an ordering of the categories and selecting a distance among them, which in most cases is not sensible. In the binary case, one could transform the response variable into 0s and 1s and build a linear regression model: this would lead to an OK model, even if some predictions could be larger than 1, and so it would be difficult to interpret the result of it as a probability of classifying the input as belonging to class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-ability",
   "metadata": {
    "heading_collapsed": "true",
    "tags": []
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "growing-microwave",
   "metadata": {},
   "source": [
    "Logistic regression models the probability that $Y$ belongs to a particular category, e.g. $\\mathbb{P}(\\text{default=yes}|\\text{balance}) = p(\\text{balance})$ will range between 0 and 1, so, given any value for balance, a prediction for default can be made.\n",
    "\n",
    "Logistic function: $p(X) = \\dfrac{e^{\\beta_0 + \\beta_1X}}{1+e^{\\beta_0 + \\beta_1X}}$, with outputs between 0 and 1. From the definition it follows that:\n",
    "\n",
    "$$\n",
    "\\dfrac{p(X)}{1-p(X)} = e^{\\beta_0 + \\beta_1X}.\n",
    "$$\n",
    "The quantity on the left is called `odds`, and ranges from 0 to $\\infty$.\n",
    "Taking the logarithm,\n",
    "$$\n",
    "\\log\\left({\\dfrac{p(X)}{1-p(X)}}\\right) = \\beta_0 + \\beta_1X.\n",
    "$$\n",
    "The quantity on the left is called `logit` or `log-odds`, which is linear in $X$. Here, increasing $X$ by 1 unit changes the log-odds by $\\beta_1$, which does not correspond to the change in $p(X)$: that will depend on the current value of $X$, too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-vocabulary",
   "metadata": {},
   "source": [
    "### Estimating the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chemical-sterling",
   "metadata": {},
   "source": [
    "Maximum Likelihood: we try to find $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ such that plugging these estimates into the model for $p(X)$ yields a number close to one for all instances of one class, and a number close to zero for all instances of the other class. The goal is to find $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ which maximize the following function:\n",
    "\n",
    "$$\n",
    "l(\\beta_0, \\beta_1) = \\prod_{i: y_i=1}p(x_i)\\prod_{j: y_j=0}(1-p(x_j)).\n",
    "$$\n",
    "\n",
    "Then, many considerations done for the linear regression are still valid for the logistic regression, noting that the $z$-statistic replaces the $t$-statistic. The estimated intercept of the logistic regression is usually not interesting: its main purpose is to adjust the average fitted probabilities to the proportion of ones in the data.\n",
    "\n",
    "Then, prediction is performed simply using \n",
    "$\n",
    "\\hat{p}(X) = \\dfrac{e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}{1+e^{\\hat{\\beta}_0 + \\hat{\\beta}_1X}}.\n",
    "$\n",
    "For categorical input variables, one can use the same approach seen for linear regression, using dummy variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-treaty",
   "metadata": {},
   "source": [
    "### Multiple logistic regression\n",
    "\n",
    "Extends analogously to the linear regression case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-university",
   "metadata": {},
   "source": [
    "### Logistic regression for more than 2 response classes\n",
    "\n",
    "Possible, but not frequently used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-heaven",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-candle",
   "metadata": {},
   "source": [
    "Logistic regression involves directly modeling $\\mathbb{P}(Y = k|X = x)$ using the logistic function. We now consider an alternative and less direct approach to estimating these probabilities. In this alternative approach, we model the distribution of the predictors $X$ separately in each of the response classes (i.e. given $Y$), and then use Bayes’ theorem to flip these around into estimates $\\mathbb{P}(Y = k|X = x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-relation",
   "metadata": {},
   "source": [
    "We define:\n",
    "- $\\pi_k$: prior probability that an observation comes from the $k$-th class (usually this is the proportion of responses of class $k$ in the training set);\n",
    "- $f_k(x) = \\mathbb{P}(X = x| Y=k)$ the density function of $X$ for the $k$-th class.\n",
    "Then Bayes states that\n",
    "$$\n",
    "p_k(x) = \\mathbb{P}(Y=k | X=x) = \\dfrac{\\pi_kf_k(x)}{\\sum_{l=1}^{K}\\pi_lf_l(x)}.\n",
    "$$\n",
    "\n",
    "$p_k(X)$ is called posterior probability, and we want to estimate it to get as close as possible to the Bayes Classifier, which has the lowest possible error rate out of all classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-commons",
   "metadata": {},
   "source": [
    "see a clear example on https://en.wikipedia.org/wiki/Naive_Bayes_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-gross",
   "metadata": {},
   "source": [
    "### case $p=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-laptop",
   "metadata": {},
   "source": [
    "Let's assume that $f_k(x)$ is normal or Gaussian, i.e.\n",
    "$$\n",
    "f_k(x) = \\dfrac{1}{\\sqrt{2\\pi}\\sigma_k}\\exp{\\left(-\\dfrac{(x-\\mu_k)^2}{2\\sigma^2_k}\\right)}.\n",
    "$$\n",
    "\n",
    "We also assume that all variances are equal, so we will drop the $k$ subscript\n",
    "With some calculus, one can show that assigning an observation to the class for which $p_k(x)$ is largest is equivalent to assigning it to the class for which\n",
    "$$\n",
    "\\delta_k(x) = x \\cdot \\dfrac{\\mu_k}{\\sigma^2} - \\dfrac{\\mu^2_k}{2\\sigma^2} + \\log{\\pi_k}\n",
    "$$\n",
    "is largest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "athletic-browse",
   "metadata": {},
   "source": [
    "We have to estimate $\\mu_1, ..., \\mu_k$, $\\pi_1, ..., \\pi_k$ and $\\sigma^2$. LDA does it in the following way:\n",
    "\n",
    "- $\\hat{\\mu}_k = \\dfrac{1}{n_k}\\sum_{i:y_i=k}{x_i}$ ---- (average of all training observations from class $k$),\n",
    "\n",
    "- $\\hat{\\sigma}^2 = \\dfrac{1}{n-K}\\sum_{k=1}^{K}\\sum_{i:y_i=k}(x_i-\\hat{\\mu}_k)^2$ ---- (weighted average of variances for each class),\n",
    "\n",
    "- $\\pi_k = \\dfrac{n_k}{n}$ ---- (proportion of training observations of class $k$),\n",
    "\n",
    "where $n$ is the size of the training set and $n_k$ the number of training observations having $k$ as response class.\n",
    "\n",
    "So LDA assigns $X=x$ to the class for which\n",
    "$$\n",
    "\\hat{\\delta}_k(x) = x \\cdot \\dfrac{\\hat{\\mu}_k}{\\hat{\\sigma}^2} - \\dfrac{\\hat{\\mu}^2_k}{2\\hat{\\sigma}^2} + \\log{\\hat{\\pi}_k}\n",
    "$$\n",
    "is largest, and the equation is linear in $x$, whence the name."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-locking",
   "metadata": {},
   "source": [
    "### case p>1\n",
    "\n",
    "Analogously using multivariate Gaussian distribution and covariance matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "helpful-wilson",
   "metadata": {},
   "source": [
    "### Remark\n",
    "\n",
    "In a binary setting, the threshold of the LDA can be manually adjusted to prefer precision over recall or vice-vicersa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-parks",
   "metadata": {},
   "source": [
    "## Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-target",
   "metadata": {},
   "source": [
    "Like LDA, the QDA classifier results from assuming that the observations from each class are drawn from a Gaussian distribution, and plugging estimates for the parameters into Bayes’ theorem in order to perform prediction. However, unlike LDA, QDA assumes that each class has its own covariance matrix, so that the $\\delta$ function becomes quadratic in $x$.\n",
    "\n",
    "- LDA: high bias, low variance\n",
    "- QDA: low bias, high variance\n",
    "\n",
    "Roughly speaking, LDA tends to be a better bet than QDA if there are relatively few training observations and so reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance of the classifier is not a major concern, or if the assumption of a common covariance matrix for the K classes is clearly untenable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "young-spank",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-injection",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-climb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amended-stress",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invisible-reporter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
